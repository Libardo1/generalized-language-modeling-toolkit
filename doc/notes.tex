\documentclass[11pt,a4paper]{article}

\usepackage{mathtools}
\usepackage{amssymb}

\usepackage[parfill]{parskip}

\usepackage{color}

\usepackage{hyperref}

\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}

\newcommand{\Seq}{\mathcal{S}}
\newcommand{\Hist}{\mathcal{H}}
\newcommand{\SeqF}{\mathcal{S}_F}
\newcommand{\HistF}{\mathcal{H}_F}
\newcommand{\Skp}{\rule{7pt}{.5pt}}
\newcommand{\SmallSkp}{\rule{5pt}{.3pt}}

\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO}: #1}}
\newcommand{\rp}[1]{\textcolor{blue}{Rene: #1}}
%newcommand{\rp}[1]{}
\newcommand{\ls}[1]{\textcolor{magenta}{Lukas: #1}}


\title{GLMTK --- Notes}
\author{Lukas Schmelzeisen \\ \texttt{lukas@uni-koblenz.de}}
\date{\today}

\begin{document}
  \maketitle
  \tableofcontents
  \clearpage

  \section{Attribution}

  I learned about most matter described in this document during my
  conversations with \textsc{Rene Pickhardt}. Most of the formulas described
  here can be attributed to his research.

  Besides that, much stems from other language modeling research papers.

  \section{Notation}

  \begin{tabular}{ l l }
    $\Sigma^n$  & Set of all $n$-Grams \\
    $\Skp$ & Skipped Word \\
    $\Seq_i$ & $i$-th Word in Sequence \\
    $|\Seq|$ & Number of Words in Sequence \\
    $c(\Seq)$ & Absolute Count of Sequence \\
    $N_{1+}(\Seq)$ & Continuation Count of Sequence \\
    $N = c(\Skp)$ & Number of Words \\
    $V = N_{1+}(\Skp)$ & Vocabulary Size \\
  \end{tabular}

  \section{$n$-Gram Probability Estimators}

  A $n$-Gram Probability Estimator is a function $P :\Sigma^n \to [0,1]$ which
  returns the probability of a $n$-Gram \emph{Sequence} $\Seq$ for a fixed
  $n$-Gram \emph{History} $\Hist$.

  \rp{not to be confused with a $P_\Hist:\Sigma \to [0,1]$ which is what many
  papers describe as an n-gram model. I do not have a word for this object yet.
  Maybe one could call it NextWord-n-gram Model because it only focuses on the
  next word. This would be only one factor in the chainrule for marginal
  probabilities (I know the term is not defined here)}

  \rp{Even though I think that you have a clear understanding of the following
  it is not 100\% obvious from the text: I am not clear how you define a
  Sequence here. Is $|\Seq|=n$ in your case or is $|\SeqF|=n$ in your case?
  In any case we need to define that an n-gram is an n-gram is a sequence plus
  and additional count how often the sequence occured.}

  For easier handling we define the \emph{Full Sequence} as the concatenation
  of history and sequence ($\SeqF = \Hist * \Seq$), and the \emph{Full History}
  as the concatenation of history and skipped sequence
  ($\HistF = \Hist * \underbrace{\: \Skp \dotso \:}_{\mathclap{|\Seq| \: \text{many}}}$)

  An observation on the counts of $n$-Grams:
  \begin{equation}
    c(\Hist) = 0 \implies c(\HistF) = 0 \implies c(\SeqF) = 0
  \end{equation}

  For histories we define the predicate of a (un-)seen history. Note that this
  defines the empty history as ``seen'', which is a choice that was made in
  order to make the definitions and implementations of estimators more natural.
  \begin{equation}
    \begin{aligned}
      \Hist \: \mathrm{seen} &\iff \Hist = \varnothing \lor c(\Hist) \neq 0 \\
      \Hist \: \mathrm{unseen} &\iff \Hist \neq \varnothing \land c(\Hist) = 0
    \end{aligned}
  \end{equation}

  \subsection{Kinds of Probability Estimators}

  $n$-Gram probability estimators can be separated into two categories,
  according to which mathematical type of probability they implement
  \rp{I did not know there are different kinds of mathematical probabilities}:
  \emph{Conditional Probabilities} or \emph{Marginal Probabilities}. How you
  actually estimate the probability of a sequence depends on what kind of
  estimator you are using.

  $n$-Gram probability with conditional probability estimators:
  \begin{equation}
    P(w_1^n) = P(w_n | w_1^{n-1}) \cdot P(w_{n-1} \: \Skp \: | w_1^{n-2}) \dotsm P(w_1 \: \Skp \: \Skp \: \dotso)
  \end{equation}

  \rp{in the following it becomes crucial to name all probability functions
  differently. Even though they might all be calculated as a fraction or MLE
  they are indeed very different functions from different spaces. I corrected
  this in the following.}

  $n$-Gram probability with marginal probability estimators:
  \begin{equation}
    P_{marginal}(w_1^n) = P_n(w_n | w_1^{n-1}) \cdot P_{n-1}(w_{n-1} | w_1^{n-2}) \dotsm P_1(w_1)
  \end{equation}

  Conditional and marginal probabilities differ  on how they handle the case of
  an unseen history. Conditional probabilities have
  $P(\Seq \, | \, \Hist \: \mathrm{unseen}) = 0$
  while marginal probabilities have
  $P(\Seq \, | \, \Hist \: \mathrm{unseen}) = P_\mathrm{Substitute}(\Seq | \Hist)$.

  \todo{Check if this statement is true for unseen histories or just for
  $P(\Hist) = 0$.}

  \rp{Here we need to discuss what is really happening. I understand where the
  substitute probability function comes from. But I do not like this as a
  definition. If above in the marginal probability case was defined that each
  factor has to be a probability function in $\Seq$ it would be clear that if
  the functions are calculated as fractions and $\Hist$ was unseen that this
  case cannot be handled like the conditional probability case. I don't have
  internet here but I guess there is a theorem explaining exactly what has to be
  required for a product measure to be a probability function (id est explaining
  the chain rule of probability in the marginal case)}

  \rp{I still do not fully like the fact that we have a sequence and not a word
  as an argument we will predict. I understand that this is necessary for theory
  building especially with skips and rescuing conditional probabilities. Also I
  see that this comes from implementation details yet it feels somewhat
  unnatural. It could be though that I am just biased by the sheer amount of
  papers going only for a word as an argument}

  \subsection{Tests for Probability Estimators}

  In order for probability estimators to be probability measures, the
  following equations / tests should hold:

  \texttt{NGramProbabilitiesSumTest}:
  \begin{equation}
    \sum_{\Seq \in \Sigma^n} P(\Seq) = 1
  \end{equation}

  \rp{note for myself. this should be very soon theoretically "proofed" by me.
  Just also to have a proper formulation of everything in a probabilistic way}

  \texttt{FixedHistoryProbabilitiesSumTest}:
  \begin{alignat}{2}
    \intertext{Conditional:}
    \forall \Hist \in \Sigma^n:
    \enspace&(\Hist \, \Skp \enspace \mathrm{seen} &&\implies \sum_{\Seq \in \Sigma} P(\Seq | \Hist) = 1) \enspace \land \\
    \enspace&(\Hist \, \Skp \enspace \mathrm{unseen} &&\implies \sum_{\Seq \in \Sigma} P(\Seq | \Hist) = 0) \notag \\
    \intertext{Marginal:}
    \forall \Hist \in \Sigma^n:
    \enspace&\sum_{\Seq \in \Sigma} P(\Seq | \Hist) &&= 1
  \end{alignat}

  \subsection{Substitute Probability Estimators}

  Substitute Probability Estimators are used in a context where other
  probability estimators cannot use their usual algorithm to estimate the
  probability of a sequence. They then instead use $P_\mathrm{Substitute}$ to
  calculate that probability.

  \rp{better: In the case of marginal probabilities we have to define a
  probability function for each value the history can take. It now can happen
  that if a certain history value was unssen that the formula for calculation
  breaks and we do not receive a probability function. In those cases we just
  guess (as we btw do in all other cases too) a probability function}

  Let $P_\mathrm{Substitute} \in \big\{ P_\mathrm{Uniform} , P_\mathrm{AbsUnigram} , P_\mathrm{ContUnigram}\big\}$
  fixed globally at program start.
  \begin{align}
    P_\mathrm{Uniform}\big(\Seq | \Hist\big) &= \frac{1}{V} \\
    P_\mathrm{AbsUnigram}\big(\Seq | \Hist\big) &= \frac{c\big(\Seq_1\Big)}{N} \\
    P_\mathrm{ContUnigram}\big(\Seq | \Hist\big) &= \frac{N_{1+}\big(\Skp \, \Seq_1\big)}{N_{1+}\big(\Skp \, \Skp\big)}
  \end{align}

  \rp{here isbackoff missing. AbsUnigram is a special case of backoff where we
  backoff all the way to the unigram distribuion. The main reason why backoff
  methods work and why people can play with this in implementations seems to be
  that they are in the marginal setting and need a substitute function. Backing
  off seems most accurate since it comprises more knowledge / information thatn
  AbsUnigram or even uniform}

  All substitute probability estimators are marginal probability estimators.

  \rp{I do not understand how this statement is true. It could be that we have a
  different definition of marginal probability estimator in mind... btw. we need
  terms for the full product and the factors in the marginal settings. I also
  like the following defintion in the marginal setting:}

  \begin{equation}
    P_n(w_n|w_i^{n-1})= \begin{cases}
      e.g. MLE & \Hist \: \mathrm{seen}  \\
      substitute & \Hist \: \mathrm{unseen}
    \end{cases}
  \end{equation}


  \subsection{Fraction Estimators}

  \rp{the following is very nice from the point of view of abstraction and
  imiplementation detail. I am not sure if I would build up the theory from this
  angle. It seem s tempting though to be driven by implementation details
  because this ensures that every side talks about the same thing.}

  Fraction Estimators are probability estimators that have the form $\frac{n}{d}$.
  \begin{alignat}{2}
    \intertext{Conditional:}
    P_\mathrm{Frac}{\scriptstyle[n , d]}\big(\Seq | \Hist\big) &= \begin{cases}
      0\hphantom{_\mathrm{Substitute}(\Seq | \Hist)} & \text{if} \enspace \Hist \: \mathrm{unseen} \lor d = 0 \\
      \frac{n}{d} & \text{else}
    \end{cases} \\
    \intertext{Marginal:}
    P_\mathrm{Frac}{\scriptstyle[n , d]}\big(\Seq | \Hist\big) &= \begin{cases}
      P_\mathrm{Substitute}(\Seq | \Hist) & \text{if} \enspace \Hist \: \mathrm{unseen} \lor d = 0 \\
      \frac{n}{d} & \text{else}
    \end{cases}
  \end{alignat}

  \subsubsection{MaximumLikelihoodEstimator}

  \begin{equation}
    P_\mathrm{MLE}\big(\Seq | \Hist\big) = P_\mathrm{Frac}{\scriptstyle[c(\SeqF) , c(\HistF)]}\big(\Seq | \Hist\big)
  \end{equation}
  \todo{If history is empty we use $c(\Skp)$ instead of $c(\HistF)$.}

  \subsubsection{``False''MaximumLikelihoodEstimator}

  \todo{Find better name.}

  FMLE only works in the marginal probability setting.

  \begin{equation}
    P_\mathrm{FMLE}\big(\Seq | \Hist\big) = P_\mathrm{Frac}{\scriptstyle[c(\SeqF) , c(\Hist)]}\big(\Seq | \Hist\big)
  \end{equation}
  \todo{If history is empty we use $c(\Skp)$ instead of $c(\Hist)$.}

  \subsubsection{ContinuationMaximumLikelihoodEstimator}

  \begin{equation}
    P_\mathrm{CMLE}\big(\Seq | \Hist\big) = P_\mathrm{Frac}{\scriptstyle[N_{1+}(\SmallSkp \, \SeqF) , N_{1+}(\SmallSkp \, \HistF)]}\big(\Seq | \Hist\big)
  \end{equation}

  \todo{\texttt{FixedHistoryProbabilitiesSumTest} has $\Skp \Hist \Skp$ check.}

  \subsection{Discount Estimators}

  A Discount Estimator takes any kind of fraction estimator and subtracts some
  discount from the numerator, in order to free probability mass to be used for
  smoothing. Obviously this means, that discount estimators are no longer
  probability estimators, and will not pass tests. Instead they have to be used
  in conjunction with an Interpolation Estimator. Discount estimators are still
  fraction estimators though.

  \rp{I would not be sure if we should say it like this. I understand that we
  basically always have fraction estimators in mind since everything starts with
  this and modifies and combines this in some way. still I guess discounting an
  many other methods could work with other models too}

  \begin{equation}
    P_\mathrm{Discount}{\scriptstyle[D,P_\mathrm{Frac}[n,d]]}\big(\Seq | \Hist\big) = P_\mathrm{Frac}{\scriptstyle[\max(0,n-D),d]}\big(\Seq | \Hist\big)
  \end{equation}
  With $D: \Hist \to [0,1]$.

  \subsubsection{AbsoluteDiscountEstimator}

  \begin{equation}
    P_\mathrm{AbsDiscount}{\scriptstyle[D,P_\mathrm{Frac}[n,d]]}\big(\Seq | \Hist\big) = P_\mathrm{Discount}{\scriptstyle[D,P_\mathrm{Frac}[n,d]]}\big(\Seq | \Hist\big)
  \end{equation}
  With $D \in [0,1]$.

  \subsection{Backoff Estimators}

  \begin{alignat}{2}
    \intertext{Conditional:}
    P_\mathrm{Backoff}{\scriptstyle[P_\alpha,P_\beta]}\big(\Seq | \Hist\big) = \begin{cases}
      0 & \text{if} \enspace \Hist = \varnothing \\
      \gamma(\Hist) \cdot P_\beta\big(\Seq | \hat \Hist\big)& \text{if} \enspace c(\SeqF) = 0 \\
      P_\alpha\big(\Seq | \Hist\big) & \text{else}
    \end{cases}
    \intertext{Marginal:}
    P_\mathrm{Backoff}{\scriptstyle[P_\alpha,P_\beta]}\big(\Seq | \Hist\big) = \begin{cases}
      P_\mathrm{Substitute}\big(\Seq | \Hist\big) & \text{if} \enspace \Hist = \varnothing \\
      \gamma(\Hist) \cdot P_\beta\big(\Seq | \hat \Hist\big)& \text{if} \enspace c(\SeqF) = 0 \\
      P_\alpha\big(\Seq | \Hist\big) & \text{else}
    \end{cases}
  \end{alignat}

  With $P_\alpha,P_\beta$ any probability estimators and
  \emph{backoff coefficient $\gamma$}:
  \begin{equation}
    \gamma(\Hist) = \frac{1 - \sum_{\Seq \in \Sigma : c(\Hist \Seq) > 0}P_\alpha\big(\Seq | \Hist\big)}{\sum_{\Seq \in \Sigma : c(\Hist\Seq) = 0}P_\beta\big(\Seq | \hat \Hist\big)}
  \end{equation}
  \todo{if lower sum = 0: return 0,}

  \subsection{Interpolation Estimators}

  \rp{why is there no difference in conditional and marginal case. I just see
  there is latex comments where you tried this...}

  \ls{The difference is in $P_\mathrm{Frac}$, this definition should span both
  cases although it is still buggy.}

  \begin{alignat}{2}
    &P_\mathrm{Interpol}{\scriptstyle[P_\mathrm{Discount}[D,P_\mathrm{Frac}[n,d]],P_\beta]}\big(\Seq | \Hist\big) = \\
    &\qquad\begin{cases}
      P_\mathrm{Frac}{\scriptstyle[n,d]}\big(\Seq | \Hist\big) \hspace{80pt} \text{if} \enspace \Hist = \varnothing \: \lor \: \Hist \:\text{contains only} \: \Skp\\
      P_\mathrm{Discount}{\scriptstyle[D,P_\mathrm{Frac}[n,d]]}\big(\Seq | \Hist\big) \: + \: \gamma(D,d,H) \cdot P_\beta\big(\Seq | \Hist\big) \hfill \text{else}
    \end{cases} \notag
  \end{alignat}
  %\begin{alignat}{2}
  %  \intertext{Conditional:}
  %  &P_\mathrm{Interpol}{\scriptstyle[P_\mathrm{Discount}[D,P_\mathrm{Frac}[n,d]],P_\beta]}\big(\Seq | \Hist\big) = \\
  %  &\qquad\begin{cases}
  %    0 & \text{if} \enspace \Hist = \varnothing \\
  %    P_\mathrm{Discount}{\scriptstyle[D,P_\mathrm{Frac}[n,d]]}\big(\Seq | \Hist\big) \: + \: \gamma(D,d,H) \cdot P_\beta\big(\Seq | \Hist\big) & \text{else}
  %  \end{cases} \notag
  %  \intertext{Marginal:}
  %  &P_\mathrm{Interpol}{\scriptstyle[P_\mathrm{Discount}[D,P_\mathrm{Frac}[n,d]],P_\beta]}\big(\Seq | \Hist\big) = \\
  %  &\qquad\begin{cases}
  %    P_\mathrm{Substitute}(\Seq | \Hist) & \text{if} \enspace \Hist = \varnothing \\
  %    P_\mathrm{Discount}{\scriptstyle[D,P_\mathrm{Frac}[n,d]]}\big(\Seq | \Hist\big) \: + \: \gamma(D,d,H) \cdot P_\beta\big(\Seq | \hat\Hist\big) & \text{else}
  %  \end{cases} \notag
  %\end{alignat}

  With the $P_\beta$ any probability estimator and
  \emph{interpolation coefficient $\gamma$}:
  \begin{equation}
    \gamma(D,d,H) = \begin{cases}
      0 & \text{if} \enspace d = 0 \\
      \frac{D \cdot N_{1+}(\Hist \, \SmallSkp)}{d} & \text{else}
    \end{cases}
  \end{equation}

  \subsection{Combination Estimator}

  A Combination Estimator mixes two other probability estimators.
  \begin{equation}
    P_\mathrm{Comb}{\scriptstyle[\lambda,P_\alpha,P_\beta]}\big(\Seq | \Hist\big) = \lambda \cdot P_\alpha\big(\Seq | \Hist\big) \enspace + \enspace (1 - \lambda) \cdot P_\beta\big(\Seq | \Hist\big)
  \end{equation}
  With $P_\alpha,P_\beta$ any probability estimators and $\lambda \in [0,1]$.

  \section{TODO}

  \begin{itemize}
    \item Interpol estimator still fails test for conditional case.
    \item FMLE still doesn't pass any tests.
    \item How to make \texttt{FixedHistoryProbabilitiesSumTest} with Continuation Estimators not be a mess?
    \item How to do \texttt{FixedHistoryProbabilitiesSumTest} with Combination Estimator.
    \item \rp{how to introduce skips on a general basis (also with POS?)}
  \end{itemize}

\end{document}
